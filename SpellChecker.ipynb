{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2970633e435fab4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Spell Checker Evaluation\n",
    "\n",
    "In this notebook, I will evaluate various spell-checking models on a dataset of spelling errors. The models I will use include PySpellChecker, Autocorrect, TextBlob, and Spello. I will analyze their performance based on accuracy, precision, recall, F1-score, and average edit distance.\n",
    "\n",
    "# Dataset\n",
    "\n",
    "# Dataset format\n",
    "\n",
    "Peter Norvig's Spell Errors Corpus: This dataset includes a list of spelling errors compiled from sources like Wikipedia and academic studies. It is structured as \"right word: wrong1, wrong2\" pairs, which is useful for training and evaluating spell checkers. You can download it from Norvig's repository.\n",
    "\n",
    "# More datasets\n",
    "\n",
    "Some more datasets I found include:\n",
    "- birkbeck\n",
    "- aspell\n",
    "- holbrook\n",
    "- wikipedia\n",
    "You can find these datasets at: https://www.dcs.bbk.ac.uk/~roger/corpora.html. \n",
    "\n",
    "# Importing necessaary libraries\n",
    "\n",
    "Let's import all the necessary libraries to run our code and test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef353db9a2b84f3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import editdistance\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from spellchecker import SpellChecker\n",
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "from spello.model import SpellCorrectionModel\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ae0ed0a9013aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make sure to install necessary packages for these spellcheckers with pip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3642e01c89fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ee051a83844df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a2edb65c4882",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pip install spellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cde009e624e7e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e513022bcaf869",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Additionaly, you'll need to download pre-trained model for spello, here named \"en.pkl\". In **spellChecker** directory, you'll need to create directory named **models**, and there subdirectory **spello**. Copy donwnloaded and unzipped file en.pkl into **spello**. We will later train our model and save updated version on our data to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca378209604f6225",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data reading and dictionary creating\n",
    "\n",
    "Firstly, we preprocess the data given in teh form of: \"right word: wrong1, wrong2\" pairs. We create dictionary *spell_errors*, where we for each key, correct spelling, provide items that are incorrect spellings for that word.\n",
    "Then, we create list test_data, where we split each key and its items into pairs in the form of: (incorrect_spelling, correct_spelling).\n",
    "At the end, I shuffle test_data as to keep randomness of the words we will test our models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32a131a398be32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    lines = file.readlines()\n",
    "    spell_errors = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Split each line by \":\"\n",
    "        correct_word, incorrect_words = line.split(\":\")\n",
    "        incorrect_words_list = incorrect_words.strip().split(\",\")\n",
    "        spell_errors[correct_word.strip()] = [word.strip() for word in incorrect_words_list]\n",
    "\n",
    "    test_data = []\n",
    "    for correct_word, incorrect_words in spell_errors.items():\n",
    "        for incorrect_word in incorrect_words:\n",
    "            test_data.append((incorrect_word, correct_word))\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27138e973d5852",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Spellcheckers\n",
    "\n",
    "1.Pyspell\n",
    "\n",
    "PySpell provides functionality for spell checking and sentence correction. It uses a dictionary of words and the Levenshtein distance algorithm to suggest corrections.\n",
    "\n",
    "2.Autocorrect\n",
    "\n",
    "Autocorrect is Python library used for precisely spellchecking words.\n",
    "\n",
    "3.Textblob\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "4.Spello\n",
    "\n",
    "Spello is a spellcorrection model built with combination of two models, Phoneme and Symspell Phoneme Model uses Soundex algo in background and suggests correct spellings using phonetic concepts to identify similar sounding words. On the other hand, Symspell Model uses concept of edit-distance in order to suggest correct spellings. Spello get's you best of both, taking into consideration context of the word as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd97beace907f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function Definitions\n",
    "\n",
    "We will define several functions to handle the spell correction process, evaluate the models, and preprocess the dataset.\n",
    "\n",
    "*spello_correction* will be our function for returning correct spelling using Spello model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56198689ed11e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:27.195106Z",
     "start_time": "2024-11-03T19:20:27.192031Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spello_correction(word):\n",
    "    \"\"\"Corrects the spelling of a word using the Spello model.\"\"\"\n",
    "    correction = spello_checker.spell_correct(word)\n",
    "    corrected_word = correction.get(\"spell_corrected_text\", word)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9473745bfc9dc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*safe_correction* is function for calling spellchecking tools based on the model we want to evaluate. I made sure that in case there's no return value(return value type is *None*) I return unchanged original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93205b0642d9012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:28.554142Z",
     "start_time": "2024-11-03T19:20:27.196115Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_correction(spell_checker, word):\n",
    "    \"\"\"Applies the specified spell checker to a given word.\"\"\"\n",
    "    if spell_checker == pyspell_checker:\n",
    "        corrected_word = pyspell_checker.correction(word)\n",
    "    elif spell_checker == autocorrect_checker:\n",
    "        corrected_word = autocorrect_checker(word)\n",
    "    elif spell_checker == 'textblob':\n",
    "        corrected_word = str(TextBlob(word).correct())\n",
    "    elif spell_checker == 'spello':\n",
    "        corrected_word = spello_correction(word)\n",
    "    else:\n",
    "        corrected_word = word\n",
    "        \n",
    "    return corrected_word if corrected_word is not None else word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe5e562934c6d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Metrics and parameters\n",
    "\n",
    "For measuring model accuracy, I adopted different parameters. For reference I used the paper: https://gerhard.pro/files/PublicationVanHuyssteenEiselenPuttkammer2004.pdf.\n",
    "\n",
    "# Spell Checker Evaluation Metrics\n",
    "\n",
    "To evaluate the performance of a spell checker, we use several metrics organized into three groups:\n",
    "\n",
    "---\n",
    "\n",
    "## Group 1: Classification Metrics\n",
    "\n",
    "1. **Recall**  \n",
    "   Measures the proportion of misspelled words that were correctly identified as misspelled by the spell checker.\n",
    "\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "   \\]\n",
    "\n",
    "2. **Precision**  \n",
    "   Measures the proportion of words marked as misspelled by the spell checker that were actually incorrect.\n",
    "\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "   \\]\n",
    "\n",
    "3. **Identifying Accuracy**  \n",
    "   Overall accuracy of the spell checker in identifying both correct and incorrect words.\n",
    "\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "   \\]\n",
    "\n",
    "4. **Average Edit Distance**  \n",
    "   The average number of changes needed to convert the spell checker’s output to the correct word.\n",
    "\n",
    "   \\[\n",
    "   \\text{Average Edit Distance} = \\frac{\\sum (\\text{Edit Distance for each word})}{\\text{Total Predictions}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Group 2: Correction Metrics\n",
    "\n",
    "1. **Percent of Words Invalid After Checker Work**  \n",
    "   Measures the percentage of words that remain invalid after being processed by the spell checker.\n",
    "\n",
    "   \\[\n",
    "   \\text{Percent Invalid After Check} = \\frac{\\text{Invalid Words After Check}}{\\text{Total Predictions}} \\times 100\n",
    "   \\]\n",
    "\n",
    "2. **Percent of Correctly Fixed Misspellings**  \n",
    "   The percentage of originally misspelled words that were corrected by the spell checker.\n",
    "\n",
    "   \\[\n",
    "   \\text{Percent Correct Fixes} = \\frac{\\text{Correct Fixes}}{\\text{Total Misspelled Words}} \\times 100\n",
    "   \\]\n",
    "\n",
    "3. **Percent of Non-fixed Misspellings with Right Correction in Top-5**  \n",
    "   Measures the percentage of misspelled words for which the correct spelling was among the top 5 suggestions, even if it wasn’t the final correction.\n",
    "\n",
    "   \\[\n",
    "   \\text{Percent Top-5 Fixes} = \\frac{\\text{Top-5 Correct Fixes}}{\\text{Total Misspelled Words}} \\times 100\n",
    "   \\]\n",
    "\n",
    "4. **Percent of Broken Valid Words**  \n",
    "   Measures the percentage of words that were originally correct but were incorrectly changed by the spell checker.\n",
    "\n",
    "   \\[\n",
    "   \\text{Percent Broken Valid Words} = \\frac{\\text{Broken Valid Words}}{\\text{Total Valid Words}} \\times 100\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Group 3: Speed Metric\n",
    "\n",
    "1. **Checker Work Speed**  \n",
    "   Measures the processing speed of the spell checker in terms of words per second.\n",
    "\n",
    "   \\[\n",
    "   \\text{Speed} = \\frac{\\text{Total Predictions}}{\\text{Total Time (seconds)}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Definitions of Key Terms:\n",
    "\n",
    "- **True Positives (TP)**: Misspelled words that the spell checker correctly identifies as incorrect.\n",
    "- **False Positives (FP)**: Correct words that the spell checker incorrectly identifies as misspelled.\n",
    "- **True Negatives (TN)**: Correct words that the spell checker correctly identifies as correct.\n",
    "- **False Negatives (FN)**: Misspelled words that the spell checker incorrectly identifies as correct.\n",
    "- **Edit Distance**: The minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into another.\n",
    "\n",
    "---\n",
    "\n",
    "These metrics provide a comprehensive view of a spell checker’s effectiveness, covering its ability to detect errors, correct them, and maintain processing efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28591386dc0b173d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:28.563695Z",
     "start_time": "2024-11-03T19:20:28.555151Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import editdistance\n",
    "\n",
    "\n",
    "def evaluate_spell_checker(spellchecker, test_data):\n",
    "    \"\"\"Evaluates the performance of a spell checker on the test data.\"\"\"\n",
    "    correct_count = 0\n",
    "    total_predictions = len(test_data)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for incorrect_word, correct_word in test_data:\n",
    "        predicted_word = safe_correction(spellchecker, incorrect_word)\n",
    "        y_true.append(correct_word)\n",
    "        y_pred.append(predicted_word)\n",
    "\n",
    "        if predicted_word == correct_word:\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "    total_edit_distance = 0\n",
    "    for true_word, pred_word in zip(y_true, y_pred):\n",
    "        distance = editdistance.eval(true_word, pred_word)\n",
    "        total_edit_distance += distance\n",
    "\n",
    "    avg_edit_distance = total_edit_distance / total_predictions\n",
    "    print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb9b031aff5bda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load and Prepare the Dataset\n",
    "\n",
    "Now we will load the dataset containing spelling errors and prepare it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22148cf273fae98f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:29.080582Z",
     "start_time": "2024-11-03T19:20:28.564705Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspell-errors.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m incorrect_word \u001b[38;5;129;01min\u001b[39;00m incorrect_words:\n\u001b[0;32m     18\u001b[0m         test_data\u001b[38;5;241m.\u001b[39mappend((incorrect_word, correct_word))\n\u001b[1;32m---> 19\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m(test_data)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_data\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(\"spell-errors.txt\", \"r\") as file:\n",
    "    test_data = preprocess(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf9d732b52352f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize Spell Checkers\n",
    "\n",
    "Next, we will initialize each of the spell-checking models we plan to evaluate.\n",
    "**IMPORTANT**: Make sure to provide correct paths to the model *\"en.pkl\"* for your computer, and correct paths for saving new trained spello model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2ba4ec597346f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-03T19:20:29.081592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from spello.model import SpellCorrectionModel\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checkers\n",
    "# 1. PySpellChecker\n",
    "pyspell_checker = SpellChecker()\n",
    "\n",
    "# 2. Autocorrect\n",
    "autocorrect_checker = Speller()  # Autocorrect\n",
    "\n",
    "# 3. TextBlob\n",
    "print(\"Textblob checker:\")\n",
    "\n",
    "# 4. Spello\n",
    "spello_checker = SpellCorrectionModel(language=\"en\")  # 'en' for English\n",
    "spello_checker.load('C:\\\\Users\\\\stoja\\\\OneDrive\\\\Desktop\\\\my-project-1\\\\spellChecker\\\\models\\\\spello\\\\model.pkl')\n",
    "spello_checker.config.min_length_for_spellcorrection = 3  # minimum length for correction\n",
    "spello_checker.config.max_length_for_spellcorrection = 15  # maximum length for correction\n",
    "\n",
    "# Prepare training data for Spello\n",
    "spello_training_data = defaultdict(int)\n",
    "for incorrect_word, correct_word in test_data:\n",
    "    spello_training_data[correct_word] += 1\n",
    "\n",
    "# Train the Spello model\n",
    "spello_checker.train(spello_training_data)\n",
    "spello_checker.save('C:\\\\Users\\\\stoja\\\\OneDrive\\\\Desktop\\\\my-project-1\\\\spellChecker\\\\models\\\\spello')\n",
    "spello_checker.load('C:\\\\Users\\\\stoja\\\\OneDrive\\\\Desktop\\\\my-project-1\\\\spellChecker\\\\models\\\\spello\\\\model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c312ea161f909e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluate the Spell Checkers\n",
    "\n",
    "Finally, we will evaluate each spell checker on a subset of the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bb90d6f927245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:29.082763Z",
     "start_time": "2024-11-03T19:20:29.082763Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate Pyspell model\n",
    "print(\"Pyspell checker:\")\n",
    "evaluate_spell_checker(pyspell_checker, test_data[:1000])\n",
    "# Evaluate Autocorrect model\n",
    "print(\"Autocorrect checker:\")\n",
    "evaluate_spell_checker(autocorrect_checker, test_data[:1000])\n",
    "# Evaluate Textblob model\n",
    "evaluate_spell_checker('textblob', test_data[:1000])\n",
    "# Evaluate Spello model\n",
    "print(\"Spello checker:\")\n",
    "evaluate_spell_checker('spello', test_data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0253131763f26",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results\n",
    "\n",
    "For each of the models, I got following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a89ccf662377a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pyspell checker:\n",
    "Precision: 1.00\n",
    "Recall: 0.66\n",
    "F1-Score: 0.80\n",
    "Identifying Accuracy: 0.66\n",
    "Percent of invalid words after checker: 66.00%\n",
    "Percent of correctly fixed misspellings: 34.00%\n",
    "Percent of non-fixed misspellings with correct suggestion in top-5: 0.00%\n",
    "Percent of broken valid words: 0.00%\n",
    "Speed (words/sec): 1.48\n",
    "--- \n",
    "Autocorrect checker:\n",
    "Precision: 1.00\n",
    "Recall: 0.67\n",
    "F1-Score: 0.80\n",
    "Identifying Accuracy: 0.67\n",
    "Percent of invalid words after checker: 67.00%\n",
    "Percent of correctly fixed misspellings: 33.00%\n",
    "Percent of non-fixed misspellings with correct suggestion in top-5: 0.00%\n",
    "Percent of broken valid words: 0.00%\n",
    "Speed (words/sec): 9.49\n",
    "---\n",
    "Textblob checker:\n",
    "Precision: 1.00\n",
    "Recall: 0.69\n",
    "F1-Score: 0.82\n",
    "Identifying Accuracy: 0.69\n",
    "Percent of invalid words after checker: 69.00%\n",
    "Percent of correctly fixed misspellings: 31.00%\n",
    "Percent of non-fixed misspellings with correct suggestion in top-5: 0.00%\n",
    "Percent of broken valid words: 0.00%\n",
    "Speed (words/sec): 2.62\n",
    "---\n",
    "Spello training started..\n",
    "Symspell training started ...\n",
    "Phoneme training started ...\n",
    "Spello training completed successfully ...\n",
    "Precision: 1.00\n",
    "Recall: 0.53\n",
    "F1-Score: 0.69\n",
    "Identifying Accuracy: 0.53\n",
    "Percent of invalid words after checker: 53.00%\n",
    "Percent of correctly fixed misspellings: 47.00%\n",
    "Percent of non-fixed misspellings with correct suggestion in top-5: 0.00%\n",
    "Percent of broken valid words: 0.00%\n",
    "Speed (words/sec): 529.83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc6e4012c15201",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on the results, we can conclude that:\n",
    "\n",
    "# Spell Checker Evaluation Summary\n",
    "\n",
    "This notebook evaluates the performance of four different spell checkers: **Pyspell**, **Autocorrect**, **TextBlob**, and **Spello**. Based on various metrics, we analyze each spell checker's strengths and weaknesses, organized by categories.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Classification Metrics\n",
    "\n",
    "| Spell Checker | Precision | Recall | F1-Score | Identifying Accuracy |\n",
    "|---------------|-----------|--------|----------|-----------------------|\n",
    "| Pyspell       | 1.00      | 0.66   | 0.80     | 0.66                  |\n",
    "| Autocorrect   | 1.00      | 0.67   | 0.80     | 0.67                  |\n",
    "| TextBlob      | 1.00      | 0.69   | 0.82     | 0.69                  |\n",
    "| Spello        | 1.00      | 0.53   | 0.69     | 0.53                  |\n",
    "\n",
    "**Observations**:\n",
    "- **Precision**: All spell checkers achieved perfect precision, meaning they accurately marked misspellings without mistakenly flagging correct words.\n",
    "- **Recall**: TextBlob has the highest recall, indicating it captures more misspellings than the others. Spello’s lower recall suggests it misses more errors.\n",
    "- **F1-Score**: TextBlob has the best F1-score (0.82), balancing precision and recall well. Spello’s F1-score is the lowest due to its lower recall.\n",
    "- **Identifying Accuracy**: TextBlob’s accuracy is also the highest at 0.69, followed closely by Autocorrect and Pyspell, with Spello being the least accurate.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Correction Metrics\n",
    "\n",
    "| Spell Checker | % Invalid Words After Check | % Correct Fixes | % Top-5 Fixes | % Broken Valid Words |\n",
    "|---------------|-----------------------------|-----------------|---------------|-----------------------|\n",
    "| Pyspell       | 66.00%                      | 34.00%          | 0.00%         | 0.00%                 |\n",
    "| Autocorrect   | 67.00%                      | 33.00%          | 0.00%         | 0.00%                 |\n",
    "| TextBlob      | 69.00%                      | 31.00%          | 0.00%         | 0.00%                 |\n",
    "| Spello        | 53.00%                      | 47.00%          | 0.00%         | 0.00%                 |\n",
    "\n",
    "**Observations**:\n",
    "- **Percent of Invalid Words After Check**: Spello leaves only 53% of words invalid, suggesting it corrects more misspellings than the other tools.\n",
    "- **Percent of Correctly Fixed Misspellings**: Spello correctly fixes 47% of misspelled words, higher than the other checkers.\n",
    "- **Percent of Non-Fixed Misspellings with Correct Suggestion in Top-5**: All checkers scored 0%, indicating that none provided the correct suggestion in their top-5 list for unfixed errors.\n",
    "- **Percent of Broken Valid Words**: All checkers scored 0%, showing they did not mistakenly alter any correctly spelled words, making them reliable for preserving valid words.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Speed Metric\n",
    "\n",
    "| Spell Checker | Speed (Words/Second) |\n",
    "|---------------|----------------------|\n",
    "| Pyspell       | 1.48                 |\n",
    "| Autocorrect   | 9.49                 |\n",
    "| TextBlob      | 2.62                 |\n",
    "| Spello        | 529.83               |\n",
    "\n",
    "**Observations**:\n",
    "- **Speed**: Spello is by far the fastest, processing 529.83 words per second, making it ideal for high-volume text processing. Autocorrect is the next fastest, though significantly slower than Spello.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "### TextBlob\n",
    "- **Best for Detection**: With the highest recall, F1-score, and identifying accuracy, TextBlob is most effective at identifying misspelled words. \n",
    "- **Trade-Off**: While it detects well, it doesn’t perform as well in fixing errors, with a higher percentage of invalid words left after checking.\n",
    "\n",
    "### Spello\n",
    "- **Best for Correction and Speed**: Spello excels in correcting misspelled words and has the lowest percentage of invalid words remaining. It’s also incredibly fast, handling over 500 words per second.\n",
    "- **Trade-Off**: Spello has lower recall and identifying accuracy, so it may miss some misspellings. However, for tasks prioritizing correction over detection, Spello is a strong choice.\n",
    "\n",
    "### Autocorrect and Pyspell\n",
    "- **Solid Performance**: These tools perform similarly, with high precision and decent identifying accuracy. However, they don’t correct as many misspellings as Spello and are slower than both Spello and TextBlob.\n",
    "- **Trade-Off**: Autocorrect and Pyspell are reliable but don’t outperform TextBlob in detection or Spello in correction.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "- Use **TextBlob** if detection accuracy (identifying misspellings) is the primary goal.\n",
    "- Use **Spello** if you need a fast spell checker that corrects a high percentage of misspellings.\n",
    "- **Autocorrect** and **Pyspell** are good alternatives but may not outperform the other options in detection or correction efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992867d66f47d25",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Additional model\n",
    "\n",
    "I tried implementing Transformers-based T5 model but didn't finish it. Here I provide also code I wrte for that model, in hope that I will succesfully implement it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995afa87176dcaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:20:29.083843Z",
     "start_time": "2024-11-03T19:20:29.083843Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import T5Config\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "class T5ModelSpellcheck:\n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        model_inputs = self.tokenizer(\n",
    "            examples[\"input_text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the target text\n",
    "        labels = self.tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    def train(self, train_data):\n",
    "        # Format training data\n",
    "        formatted_data = [\n",
    "            {\n",
    "                \"input_text\": f\"correct: {incorrect}\",\n",
    "                \"target_text\": correct\n",
    "            }\n",
    "            for incorrect, correct in train_data if incorrect and correct\n",
    "        ]\n",
    "\n",
    "        # Convert to Dataset\n",
    "        dataset = Dataset.from_dict({\n",
    "            \"input_text\": [item[\"input_text\"] for item in formatted_data],\n",
    "            \"target_text\": [item[\"target_text\"] for item in formatted_data]\n",
    "        })\n",
    "\n",
    "        # Apply tokenization to the dataset\n",
    "        tokenized_dataset = dataset.map(self.tokenize_function, batched=True)\n",
    "\n",
    "        # Split dataset into training and testing sets\n",
    "        train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./t5_pretrained\",\n",
    "            eval_strategy=\"epoch\",  # Update this line\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            remove_unused_columns=False,\n",
    "            logging_dir='./logs',  # Optional: specify where to store logs\n",
    "            logging_steps=10,  # Optional: log every 10 steps\n",
    "            #load_best_model_at_end=True  # Optional: load best model after training\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_test_split[\"train\"],\n",
    "            eval_dataset=train_test_split[\"test\"],\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.model.save_pretrained(\"./t5_pretrained\")\n",
    "        self.tokenizer.save_pretrained(\"./t5_pretrained\")\n",
    "\n",
    "    def t5_load_model(self, model_dir=\"./t5_pretrained\"):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    def t5_correct(self, word):\n",
    "        input_text = f\"correct: {word}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs)\n",
    "\n",
    "        corrected_word = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return corrected_word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
